{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# changing core directory\n",
        "import os, sys\n",
        "dir2 = os.path.abspath('')\n",
        "dir1 = os.path.dirname(dir2)\n",
        "if not dir1 in sys.path:\n",
        "    sys.path.append(dir1)\n",
        "os.chdir('..')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fjZ8TtF8mKQ6"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import ndcg_score\n",
        "import lightgbm as lgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "from src.data import get_sequences, load_data, download_movielens1m, ValSASRecDataset, TrainSASRecDataset\n",
        "from src.utils import fix_seed\n",
        "from src.model import SASRec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 0\n",
        "max_len = 200\n",
        "batch_size = 128\n",
        "embedding_size = 64\n",
        "\n",
        "n_ext_users = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "  \n",
        "fix_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiq6wFeEnPCo"
      },
      "source": [
        "# Подгрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QdgWFOUmQnd",
        "outputId": "1ddbecea-3525-4c67-befb-c4776ccbc05c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset already exists.\n"
          ]
        }
      ],
      "source": [
        "# Скачиваем и загружаем данные\n",
        "download_movielens1m(\"data/movielens_1m\")\n",
        "ratings = load_data(\"data/movielens_1m\")\n",
        "\n",
        "ratings = ratings[ratings[\"rating\"] > 3.5]\n",
        "\n",
        "# Подготовка данных\n",
        "num_users = ratings['user_id'].nunique()\n",
        "num_items = ratings['item_id'].nunique()\n",
        "\n",
        "user2id = {val:i for i, val in enumerate(ratings['user_id'].unique())}\n",
        "item2id = {val:i+1 for i, val in enumerate(ratings['item_id'].unique())}\n",
        "\n",
        "ratings['user_id'] = ratings['user_id'].map(user2id)\n",
        "ratings['item_id'] = ratings['item_id'].map(item2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 4, 1])\n",
            "torch.Size([2, 3, 4, 1]) torch.Size([2, 3, 4, 5])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 5])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bs, seq_len, n_ext_users, emb_size = 2, 3, 4, 5\n",
        "\n",
        "obj = torch.rand([bs, seq_len, n_ext_users, emb_size])\n",
        "int_obj = torch.rand([bs, seq_len, emb_size])\n",
        "\n",
        "dot_prod = torch.matmul(obj, int_obj.unsqueeze(-2).transpose(-2, -1))\n",
        "print(dot_prod.shape)\n",
        "softmax_dot_prod = nn.functional.softmax(dot_prod, -2)\n",
        "print(softmax_dot_prod.shape, obj.shape)\n",
        "agg_embeddings = (softmax_dot_prod * obj).sum(dim=2)\n",
        "agg_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5]) torch.Size([5])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 10])"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "int_obj_one = int_obj[0, 0]\n",
        "ext_obj_one = obj[0, 0]\n",
        "# int_obj_one.tile(obj.shape[2], 2).shape, int_obj.shape\n",
        "\n",
        "print(int_obj_one.tile((len(ext_obj_one), 1)).shape, int_obj_one.shape)\n",
        "\n",
        "int_obj.unsqueeze(2).tile((1, 1, obj.shape[2], 1)).shape, int_obj.shape,\n",
        "\n",
        "concated = torch.cat((ext_obj_one, int_obj_one.tile((len(ext_obj_one), 1))), axis=1)\n",
        "concated.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NSrn6M5mVF-",
        "outputId": "a8d30a89-c0f2-4f18-a543-7405c7882872"
      },
      "outputs": [],
      "source": [
        "# # Создаем последовательности\n",
        "# sequences = []\n",
        "# times = []\n",
        "# for user_id, group in ratings.groupby('user_id'):\n",
        "#     group = group.sort_values('timestamp')\n",
        "#     user_seq = group['item_id'].tolist()\n",
        "#     user_times = group['timestamp'].astype(int).tolist()\n",
        "#     sequences.append(user_seq)\n",
        "#     times.append(user_times)\n",
        "\n",
        "# # Разделяем на train/val\n",
        "# split_idx = int(0.8 * len(sequences))\n",
        "# train_sequences = sequences[:split_idx]\n",
        "# val_sequences = sequences[split_idx:]\n",
        "\n",
        "# train_times = times[:split_idx]\n",
        "# val_times = times[split_idx:]\n",
        "\n",
        "# all_users = ratings['user_id'].unique()\n",
        "# train_users, val_users = all_users[:split_idx], all_users[split_idx:]\n",
        "# print(len(train_users), len(val_users))\n",
        "\n",
        "(train_sequences, val_sequences), (train_times, val_times), (train_users, val_users) = get_sequences(ratings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umenI8xbnR3j"
      },
      "source": [
        "# Обучение нейронки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aAOIKGbTmXlo"
      },
      "outputs": [],
      "source": [
        "# Создание датасета для SASRec\n",
        "\n",
        "# class TrainSASRecDataset(Dataset):\n",
        "#     def __init__(self, sequences, times, max_len=50):\n",
        "#         self.sequences = sequences\n",
        "#         self.times = times\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sequences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         seq = self.sequences[idx]\n",
        "#         time = self.times[idx]\n",
        "#         if len(seq) > self.max_len:\n",
        "#             seq = seq[-self.max_len:]\n",
        "#             time = time[-self.max_len:]\n",
        "#         else:\n",
        "#             seq = [0] * (self.max_len - len(seq)) + seq\n",
        "#             time = [0] * (self.max_len - len(time)) + time\n",
        "#         input_seq = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "#         input_times = torch.tensor(time[:-1], dtype=torch.long)\n",
        "#         target = torch.tensor(seq[1:], dtype=torch.long)\n",
        "#         return (input_seq, input_times), target\n",
        "\n",
        "\n",
        "# class ValSASRecDataset(Dataset):\n",
        "#     def __init__(self, sequences, times, max_len=50):\n",
        "#         self.sequences = sequences\n",
        "#         self.times = times\n",
        "#         self.max_len = max_len\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.sequences)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         seq = self.sequences[idx]\n",
        "#         time = self.times[idx]\n",
        "#         if len(seq) > self.max_len:\n",
        "#             seq = seq[-self.max_len:]\n",
        "#             time = time[-self.max_len:]\n",
        "#         else:\n",
        "#             seq = [0] * (self.max_len - len(seq)) + seq\n",
        "#             time = [0] * (self.max_len - len(time)) + time\n",
        "#         input_seq = torch.tensor(seq[:-1], dtype=torch.long)\n",
        "#         input_times = torch.tensor(time[:-1], dtype=torch.long)\n",
        "#         target = torch.tensor(seq[-1], dtype=torch.long)\n",
        "\n",
        "#         return (input_seq, input_times), target\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = TrainSASRecDataset(train_sequences, train_times, max_len)\n",
        "val_dataset = ValSASRecDataset(val_sequences, val_times, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RlWAVuZjmeMd"
      },
      "outputs": [],
      "source": [
        "# # Модель SASRec на PyTorch\n",
        "# class SASRec(nn.Module):\n",
        "#     def __init__(self,\n",
        "#         num_items,\n",
        "#         hidden_units=64,\n",
        "#         num_heads=2,\n",
        "#         num_blocks=2,\n",
        "#         dropout_rate=0.2,\n",
        "#         max_len=200,\n",
        "#         ext_flag=False\n",
        "#     ):\n",
        "#         super(SASRec, self).__init__()\n",
        "#         self.num_items = num_items\n",
        "#         self.hidden_units = hidden_units\n",
        "#         self.max_len = max_len\n",
        "#         self.ext_flag = ext_flag\n",
        "\n",
        "#         self.item_emb = nn.Embedding(num_items + 1, hidden_units, padding_idx=0)\n",
        "#         self.pos_emb = nn.Embedding(max_len, hidden_units)\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "#         self.encoder_layers = nn.ModuleList([\n",
        "#             nn.TransformerEncoderLayer(\n",
        "#                 d_model=hidden_units,\n",
        "#                 nhead=num_heads,\n",
        "#                 dim_feedforward=hidden_units,\n",
        "#                 dropout=dropout_rate,\n",
        "#                 batch_first=True\n",
        "#             ) for _ in range(num_blocks)\n",
        "#         ])\n",
        "\n",
        "#         self.layer_norm = nn.LayerNorm(hidden_units)\n",
        "#         self.output_layer = nn.Linear(hidden_units, num_items + 1)\n",
        "\n",
        "#     def forward(self, input_seqs, timestamps=None):\n",
        "#         batch_size, seq_len = input_seqs.size()\n",
        "\n",
        "#         # Position encoding\n",
        "#         positions = torch.arange(seq_len, dtype=torch.long, device=input_seqs.device)\n",
        "#         positions = positions.unsqueeze(0).expand(batch_size, seq_len)\n",
        "\n",
        "#         # Item and position embedding\n",
        "#         item_emb = self.item_emb(input_seqs)\n",
        "#         pos_emb = self.pos_emb(positions)\n",
        "#         x = item_emb + pos_emb\n",
        "#         x = self.dropout(x)\n",
        "\n",
        "#         # Transformer encoder\n",
        "#         mask = self.generate_square_subsequent_mask(seq_len).to(input_seqs.device)\n",
        "#         for layer in self.encoder_layers:\n",
        "#             x = layer(x, mask)\n",
        "\n",
        "#         x = self.layer_norm(x)\n",
        "\n",
        "#         if self.ext_flag:\n",
        "#             ext_context = self.get_external_features(timestamps).to(x.device)\n",
        "#             extended_data = torch.cat([x, ext_context], dim=2)\n",
        "#             output = self.ext_head(extended_data)\n",
        "#         else:\n",
        "#             output = self.output_layer(x)\n",
        "\n",
        "#         return output\n",
        "\n",
        "#     def generate_square_subsequent_mask(self, sz):\n",
        "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "#         return mask\n",
        "\n",
        "#     def get_embeddings(self, input_seqs):\n",
        "#         batch_size, seq_len = input_seqs.size()\n",
        "\n",
        "#         positions = torch.arange(seq_len, dtype=torch.long, device=input_seqs.device)\n",
        "#         positions = positions.unsqueeze(0).expand(batch_size, seq_len)\n",
        "\n",
        "#         item_emb = self.item_emb(input_seqs)\n",
        "#         pos_emb = self.pos_emb(positions)\n",
        "#         x = item_emb + pos_emb\n",
        "\n",
        "#         mask = self.generate_square_subsequent_mask(seq_len).to(input_seqs.device)\n",
        "#         for layer in self.encoder_layers:\n",
        "#             x = layer(x, mask)\n",
        "\n",
        "#         x = self.layer_norm(x)\n",
        "#         return x\n",
        "\n",
        "#     def freeze(self):\n",
        "#         for param in self.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         print(\"Все слои сети заморожены.\")\n",
        "\n",
        "#     def add_external_features(self, ext_features):\n",
        "#         self.ext_flag = True\n",
        "#         self.freeze()\n",
        "#         self.ext_head = nn.Linear(self.hidden_units*2, num_items + 1)\n",
        "#         self.time_list = ext_features[0]\n",
        "#         self.ext_embeddings = ext_features[1]\n",
        "\n",
        "#     def get_external_features(self, timestamps):\n",
        "#         bs, seq_len = timestamps.shape\n",
        "#         timestamps = timestamps.reshape(-1).cpu().detach().numpy()\n",
        "\n",
        "#         ext_ids = np.searchsorted(self.time_list, timestamps, side='right') - 1\n",
        "#         ext_context = self.ext_embeddings[ext_ids]\n",
        "#         ext_context = torch.tensor(ext_context, dtype=torch.float32).reshape(bs, seq_len, -1)\n",
        "#         return ext_context\n",
        "\n",
        "#     # ----------------------------------------\n",
        "#     # def add_external_features(self, ext_features):\n",
        "#     #     self.ext_flag = True\n",
        "#     #     self.freeze()\n",
        "#     #     self.ext_head = nn.Linear(self.hidden_units*2, num_items + 1)\n",
        "#     #     self.ext_features = ext_features\n",
        "#     #     self.ext_features['timestamp'] = self.ext_features['timestamp'].astype(int)\n",
        "\n",
        "#     # def get_external_features(self, timestamps):\n",
        "#     #     bs, seq_len = timestamps.shape\n",
        "#     #     timestamps = timestamps.reshape(-1)\n",
        "#     #     ext_context = []\n",
        "#     #     for time in timestamps:\n",
        "#     #         ext_context.append(self.get_one_ext_represent(time))\n",
        "\n",
        "#     #     ext_context = torch.tensor(ext_context, dtype=torch.float32)\n",
        "#     #     ext_context = ext_context.reshape(bs, seq_len, -1)\n",
        "#     #     return ext_context\n",
        "\n",
        "#     # def get_one_ext_represent(self, time):\n",
        "#     #     context_info = self.ext_features[self.ext_features['timestamp'] <= time.cpu().detach().numpy()]\n",
        "#     #     if len(context_info) > 0:\n",
        "#     #         context_info = context_info.iloc[-1]\n",
        "#     #         mean_context = context_info['mean_embedding']\n",
        "#     #         return mean_context\n",
        "#     #     else:\n",
        "#     #         return np.zeros(self.hidden_units)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fKdRlSyFme6z"
      },
      "outputs": [],
      "source": [
        "def calculate_ndcg(model, dataloader, device, k=10):\n",
        "    model.eval()\n",
        "\n",
        "    # Вместо накопления всех предсказаний, будем вычислять NDCG по батчам\n",
        "    ndcg_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (input_seqs, input_times), targets in dataloader:\n",
        "            input_seqs, input_times, targets = input_seqs.to(device), input_times.to(device), targets.to(device)\n",
        "            outputs = model(input_seqs, input_times)\n",
        "            last_outputs = outputs[:, -1, :]\n",
        "\n",
        "            # Преобразуем в numpy\n",
        "            predictions = last_outputs.cpu().numpy()\n",
        "            targets_np = targets.cpu().numpy()\n",
        "\n",
        "            # Создаем матрицу релевантности только для текущего батча\n",
        "            n_items = predictions.shape[1]\n",
        "            relevance = np.zeros((len(targets_np), n_items))\n",
        "            relevance[np.arange(len(targets_np)), targets_np] = 1\n",
        "\n",
        "            # Вычисляем NDCG для текущего батча\n",
        "            try:\n",
        "                batch_ndcg = ndcg_score(relevance, predictions, k=k)\n",
        "                ndcg_scores.append(batch_ndcg)\n",
        "            except MemoryError:\n",
        "                # Если батч слишком большой, разбиваем его на подбатчи\n",
        "                sub_batch_size = 100\n",
        "                for i in range(0, len(targets_np), sub_batch_size):\n",
        "                    end_idx = min(i + sub_batch_size, len(targets_np))\n",
        "                    sub_relevance = relevance[i:end_idx]\n",
        "                    sub_predictions = predictions[i:end_idx]\n",
        "                    sub_ndcg = ndcg_score(sub_relevance, sub_predictions, k=k)\n",
        "                    ndcg_scores.append(sub_ndcg)\n",
        "\n",
        "    # Возвращаем среднее значение NDCG по всем батчам\n",
        "    return np.mean(ndcg_scores)\n",
        "\n",
        "# Обучение модели SASRec\n",
        "def train_sasrec(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
        "    model.to(device)\n",
        "    best_ndcg = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, ((input_seqs, input_times), targets) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            input_seqs, input_times, targets = input_seqs.to(device), input_times.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_seqs, input_times)\n",
        "\n",
        "            # Получаем предсказания для последнего элемента в последовательности\n",
        "            # last_outputs = outputs[:, -1, :]\n",
        "            #loss = criterion(last_outputs, targets)\n",
        "            outputs, targets = outputs.reshape(-1, num_items+1), targets.reshape(-1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Валидация\n",
        "        val_ndcg = calculate_ndcg(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}, Val NDCG@{10}: {val_ndcg:.4f}\")\n",
        "\n",
        "        # Сохраняем лучшую модель\n",
        "        if val_ndcg > best_ndcg:\n",
        "            best_ndcg = val_ndcg\n",
        "            torch.save(model.state_dict(), \"best_sasrec_model.pth\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B1shKWHmlvi",
        "outputId": "982e1b6d-03d0-427b-9684-b769b409587a"
      },
      "outputs": [],
      "source": [
        "# Инициализируем модель\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SASRec(num_items, embedding_size=embedding_size, max_len=max_len).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # # Обучаем модель\n",
        "# model = train_sasrec(model, train_loader, val_loader, optimizer, criterion, device, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePSHAAFAmn71",
        "outputId": "aded20ee-4701-481b-bee3-161a7df882ed"
      },
      "outputs": [],
      "source": [
        "# # Загружаем лучшую модель\n",
        "# model.load_state_dict(torch.load(\"best_sasrec_model.pth\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr0y4JTknWaA"
      },
      "source": [
        "# Аггрегации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB5yc5gWnZT-",
        "outputId": "252dc037-3724-4cdc-a55f-a8821e0d1a07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загружаем лучшую модель\n",
        "model.load_state_dict(torch.load(\"best_sasrec_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lshUPRuWakoV",
        "outputId": "aae906cc-7344-4244-be91-0db5592e492a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing users: 100%|██████████| 10/10 [00:00<00:00, 37.92it/s]\n"
          ]
        }
      ],
      "source": [
        "user_subset = np.random.choice(train_users, n_ext_users, replace=False)\n",
        "subset_data = ratings[ratings['user_id'].isin(user_subset)].sort_values(['user_id', 'timestamp'])\n",
        "\n",
        "# Создаем словарь для хранения эмбеддингов пользователей по времени\n",
        "user_embeddings = defaultdict(list)\n",
        "#time_embeddings = defaultdict(list)\n",
        "all_timestamps = set()\n",
        "\n",
        "# Получаем эмбеддинги для каждого пользователя в каждый момент времени\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for user_id in tqdm(user_subset, desc=\"Processing users\"):\n",
        "        user_data = subset_data[subset_data['user_id'] == user_id]\n",
        "        user_items = user_data['item_id'].tolist()\n",
        "        timestamps = user_data['timestamp'].astype(int).tolist()\n",
        "\n",
        "        if len(user_items) <= model.max_len:\n",
        "            seq = [0] * (model.max_len - len(user_items)) + user_items\n",
        "            input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            embeddings = model.get_internal_embeddings(input_seq).cpu().numpy()[0, :, :]\n",
        "\n",
        "        else:\n",
        "            for i in range(0, len(user_items) + 1, model.max_len):\n",
        "                seq = user_items[i:model.max_len + i]\n",
        "                input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "                batch_emb = model.get_internal_embeddings(input_seq).cpu().numpy()[0, :, :]\n",
        "                if i > 0:\n",
        "                    embeddings = np.concatenate([embeddings, batch_emb], axis=0)\n",
        "                else:\n",
        "                    embeddings = batch_emb\n",
        "\n",
        "        for time, embed in zip(timestamps, embeddings):\n",
        "            all_timestamps.add(time)\n",
        "            user_embeddings[user_id].append((time, embed))\n",
        "            #time_embeddings[time].append((user_id, embed))\n",
        "\n",
        "all_timestamps = sorted(list(all_timestamps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregating embeddings: 100%|██████████| 356/356 [00:00<00:00, 23446.22it/s]\n"
          ]
        }
      ],
      "source": [
        "# Создаем таблицу с агрегированными эмбеддингами по времени\n",
        "# all_timestamps = sorted(set([ts for user_embs in user_embeddings.values() for ts, _ in user_embs]))\n",
        "\n",
        "agg_raw_data_embeddings = []\n",
        "\n",
        "for timestamp in tqdm(all_timestamps, desc=\"Aggregating embeddings\"):\n",
        "    # Собираем последние эмбеддинги для каждого пользователя на данный момент времени\n",
        "    current_embeddings = []\n",
        "    current_times = []\n",
        "    for user_id, embeddings_list in user_embeddings.items():\n",
        "        # Находим последний эмбеддинг пользователя до данного времени\n",
        "        user_embs_before = [(ts, emb) for ts, emb in embeddings_list if ts <= timestamp]\n",
        "        if user_embs_before:\n",
        "            # Берем последний эмбеддинг\n",
        "            time, last_emb = user_embs_before[-1]\n",
        "            current_embeddings.append(np.array(last_emb))\n",
        "            current_times.append(time)\n",
        "\n",
        "    # Агрегируем эмбеддинги всех пользователей\n",
        "    if len(current_embeddings) < n_ext_users:\n",
        "        n_embeddings = len(current_embeddings)\n",
        "        for i in range(n_ext_users - n_embeddings):\n",
        "            current_embeddings.append(np.zeros(embedding_size))\n",
        "            current_times.append(-1e7)\n",
        "    agg_raw_data_embeddings.append([timestamp, current_times, current_embeddings])\n",
        "\n",
        "\n",
        "agg_raw_data_embeddings = sorted(agg_raw_data_embeddings, key = lambda d: d[0])\n",
        "\n",
        "time_list = np.array([data[0] for data in agg_raw_data_embeddings])\n",
        "time_to_embeddings = np.array([data[1] for data in agg_raw_data_embeddings])\n",
        "ext_embeddings = np.array([data[2]  for data in agg_raw_data_embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((356,), (356, 10), (356, 10, 64))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_list.shape, time_to_embeddings.shape, ext_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Загружаем лучшую модель\n",
        "# from src.model import SASRec\n",
        "# model = SASRec(num_items, embedding_size=embedding_size, max_len=max_len).to(device)\n",
        "# model.load_state_dict(torch.load(\"best_sasrec_model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Все слои сети заморожены.\n"
          ]
        }
      ],
      "source": [
        "external_features = time_list, ext_embeddings\n",
        "\n",
        "model.add_external_features(time_list, time_to_embeddings, ext_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 38/38 [00:29<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.5699, Val NDCG@10: 0.0047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 38/38 [00:35<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Loss: 3.4089, Val NDCG@10: 0.0071\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = train_sasrec(model, train_loader, val_loader, optimizer, criterion, device, epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1630208"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "128*199*64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if self.pooling_type == \"mean\":\n",
        "#     pooled_vector = torch.Tensor(np.mean(vectors, axis=0))\n",
        "\n",
        "# elif self.pooling_type == \"max\":\n",
        "#     pooled_vector = torch.Tensor(np.max(vectors, axis=0))\n",
        "\n",
        "# elif \"attention\" in self.pooling_type:\n",
        "#     vectors = torch.Tensor(vectors).to(user_emb.device)\n",
        "\n",
        "#     if \"learnable_attention\" in self.pooling_type:\n",
        "#         if not self.learnable_attention_matrix:\n",
        "#             raise ValueError(\"Learnable attention matrix wasn't initialized!\")\n",
        "#         vectors_prep = self.learnable_attention_matrix(vectors)\n",
        "#     elif self.pooling_type == \"symmetrical_attention\":\n",
        "#         if not self.learnable_attention_matrix:\n",
        "#             raise ValueError(\"Learnable attention matrix wasn't initialized!\")\n",
        "#         vectors_prep = self.learnable_attention_matrix(vectors)\n",
        "#         user_emb = self.learnable_attention_matrix(user_emb)\n",
        "#     elif self.pooling_type == \"kernel_attention\":\n",
        "#         if not self.attention_kernel:\n",
        "#             raise ValueError(\"Attention kernel wasn't initialized!\")\n",
        "#         vectors_prep = self.attention_kernel(vectors)\n",
        "#         user_emb = self.attention_kernel(user_emb)\n",
        "#     else: vectors_prep = vectors\n",
        "#     dot_prod = vectors_prep @ user_emb.unsqueeze(0).transpose(1, 0)\n",
        "#     softmax_dot_prod = nn.functional.softmax(dot_prod, 0)\n",
        "#     if \"attention_hawkes\" in self.pooling_type:\n",
        "#         times = torch.Tensor((times - time) * self.exp_param).to(user_emb.device)\n",
        "#         times = times.unsqueeze(-1)\n",
        "#         time_part = torch.exp(times)\n",
        "#         softmax_dot_prod = softmax_dot_prod * time_part\n",
        "#     pooled_vector = (softmax_dot_prod * vectors).sum(dim=0)\n",
        "\n",
        "# elif self.pooling_type == \"exp_hawkes\":\n",
        "#     pooled_vector = np.mean(vectors * np.exp((times - time) * self.exp_param).reshape(-1,1), axis=0)\n",
        "#     pooled_vector = torch.Tensor(pooled_vector).to(user_emb.device)\n",
        "    \n",
        "# elif \"hawkes\" in self.pooling_type:\n",
        "#     if not self.hawkes_nn:\n",
        "#         raise ValueError(\"Hawkes NN wasn't initialized!\")\n",
        "    \n",
        "#     vectors = torch.Tensor(vectors).to(user_emb.device)\n",
        "#     concated = torch.cat((vectors, user_emb.tile((len(vectors), 1))), axis=1)\n",
        "#     emb_part = self.hawkes_nn(concated)\n",
        "#     times = torch.Tensor((times - time) * self.exp_param).to(user_emb.device)\n",
        "#     times = times.unsqueeze(-1)\n",
        "#     if self.pooling_type == \"learnable_hawkes\":\n",
        "#         if not self.hawkes_time_nn:\n",
        "#             raise ValueError(\"Hawkes NN for time wasn't initialized!\")\n",
        "#         time_part = self.hawkes_time_nn(times)\n",
        "#     elif self.pooling_type == \"exp_learnable_hawkes\":\n",
        "#         time_part = torch.exp(times)\n",
        "#     else:\n",
        "#         raise ValueError(\"Unsupported pooling type.\")\n",
        "#     pooled_vector = torch.sum(emb_part * time_part, axis=0) \n",
        "\n",
        "# else:\n",
        "#     raise ValueError(\"Unsupported pooling type.\")\n",
        "\n",
        "# device = next(self.parameters()).device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTtLpSioi_u4",
        "outputId": "093d8623-0c1f-4ef2-91e6-fb5c3382f86b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Aggregating embeddings: 100%|██████████| 356/356 [00:00<00:00, 13365.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# Агрегируем эмбеддинги по времени\n",
        "def aggregate_embeddings(embeddings_list, method='mean'):\n",
        "    if not embeddings_list:\n",
        "        return np.zeros(model.hidden_units)\n",
        "\n",
        "    embeddings = np.array([emb[1] for emb in embeddings_list])\n",
        "    if method == 'mean':\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    elif method == 'max':\n",
        "        return np.max(embeddings, axis=0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown aggregation method: {method}\")\n",
        "\n",
        "# Создаем таблицу с агрегированными эмбеддингами по времени\n",
        "all_timestamps = sorted(set([ts for user_embs in user_embeddings.values() for ts, _ in user_embs]))\n",
        "\n",
        "aggregated_embeddings = []\n",
        "\n",
        "for timestamp in tqdm(all_timestamps, desc=\"Aggregating embeddings\"):\n",
        "    # Собираем последние эмбеддинги для каждого пользователя на данный момент времени\n",
        "    current_embeddings = []\n",
        "    for user_id, embeddings_list in user_embeddings.items():\n",
        "        # Находим последний эмбеддинг пользователя до данного времени\n",
        "        user_embs_before = [(ts, emb) for ts, emb in embeddings_list if ts <= timestamp]\n",
        "        if user_embs_before:\n",
        "            # Берем последний эмбеддинг\n",
        "            last_emb = user_embs_before[-1][1]\n",
        "            current_embeddings.append(last_emb)\n",
        "\n",
        "    # Агрегируем эмбеддинги всех пользователей\n",
        "    if current_embeddings:\n",
        "        mean_agg = aggregate_embeddings([(timestamp, emb) for emb in current_embeddings], 'mean')\n",
        "        max_agg = aggregate_embeddings([(timestamp, emb) for emb in current_embeddings], 'max')\n",
        "\n",
        "        aggregated_embeddings.append({\n",
        "            'timestamp': timestamp,\n",
        "            'mean_embedding': mean_agg,\n",
        "            'max_embedding': max_agg\n",
        "        })\n",
        "\n",
        "# Создаем DataFrame с агрегированными эмбеддингами\n",
        "agg_emb_df = pd.DataFrame(aggregated_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EIjoyyTPnhsw",
        "outputId": "42752dc9-3c39-4ea8-fb0f-ebb9007444a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>mean_embedding</th>\n",
              "      <th>max_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>963085003000000000</td>\n",
              "      <td>[1.2166228, -0.7584174, -0.72425693, 0.8039133...</td>\n",
              "      <td>[1.2166228, -0.7584174, -0.72425693, 0.8039133...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>963085239000000000</td>\n",
              "      <td>[1.3693753, -0.7254764, -0.7280423, 0.7639481,...</td>\n",
              "      <td>[1.3693753, -0.7254764, -0.7280423, 0.7639481,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>963085379000000000</td>\n",
              "      <td>[1.2751011, -0.72293264, -0.739836, 0.72587556...</td>\n",
              "      <td>[1.2751011, -0.72293264, -0.739836, 0.72587556...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>963085444000000000</td>\n",
              "      <td>[1.304151, -0.73856103, -0.7569565, 0.7945317,...</td>\n",
              "      <td>[1.304151, -0.73856103, -0.7569565, 0.7945317,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>963085516000000000</td>\n",
              "      <td>[1.2577156, -0.7414157, -0.74677974, 0.7025273...</td>\n",
              "      <td>[1.2577156, -0.7414157, -0.74677974, 0.7025273...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            timestamp                                     mean_embedding  \\\n",
              "0  963085003000000000  [1.2166228, -0.7584174, -0.72425693, 0.8039133...   \n",
              "1  963085239000000000  [1.3693753, -0.7254764, -0.7280423, 0.7639481,...   \n",
              "2  963085379000000000  [1.2751011, -0.72293264, -0.739836, 0.72587556...   \n",
              "3  963085444000000000  [1.304151, -0.73856103, -0.7569565, 0.7945317,...   \n",
              "4  963085516000000000  [1.2577156, -0.7414157, -0.74677974, 0.7025273...   \n",
              "\n",
              "                                       max_embedding  \n",
              "0  [1.2166228, -0.7584174, -0.72425693, 0.8039133...  \n",
              "1  [1.3693753, -0.7254764, -0.7280423, 0.7639481,...  \n",
              "2  [1.2751011, -0.72293264, -0.739836, 0.72587556...  \n",
              "3  [1.304151, -0.73856103, -0.7569565, 0.7945317,...  \n",
              "4  [1.2577156, -0.7414157, -0.74677974, 0.7025273...  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agg_emb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XDpxmVCD-Dso"
      },
      "outputs": [],
      "source": [
        "agg_emb_df = agg_emb_df.sort_values('timestamp')\n",
        "time_list = agg_emb_df['timestamp'].values\n",
        "ext_embeddings = agg_emb_df['mean_embedding'].values\n",
        "ext_embeddings = np.stack(ext_embeddings)\n",
        "\n",
        "external_features = time_list, ext_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIu3CduTw8im",
        "outputId": "8fab7587-886b-4946-8c30-f1d686265d5e"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "add_external_features() missing 2 required positional arguments: 'time_to_embeddings' and 'ext_features'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_151581/2039013941.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_external_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexternal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: add_external_features() missing 2 required positional arguments: 'time_to_embeddings' and 'ext_features'"
          ]
        }
      ],
      "source": [
        "model.add_external_features(external_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfQO5umSw5tP"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = train_sasrec(model, train_loader, val_loader, optimizer, criterion, device, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ykpb7Yv_DsIV",
        "outputId": "fadf10a2-b6b6-4441-8a36-07fc535c312d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размороженные параметры: ['ext_head.weight', 'ext_head.bias']\n"
          ]
        }
      ],
      "source": [
        "def check_unfrozen_params(model):\n",
        "    unfrozen_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            unfrozen_params.append(name)\n",
        "    return unfrozen_params\n",
        "\n",
        "print(\"Размороженные параметры:\", check_unfrozen_params(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxvaU3k1w6Fc"
      },
      "source": [
        "# Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nw-erdinkJD"
      },
      "outputs": [],
      "source": [
        "# Добавляем расчет NDCG для бустинга\n",
        "def calculate_ndcg_for_boosting(model, X, y, user_ids, k=10):\n",
        "    \"\"\"\n",
        "    Вычисляет NDCG@k для модели бустинга, группируя предсказания по пользователям\n",
        "    \"\"\"\n",
        "    # Получаем предсказания модели\n",
        "    predictions = model.predict(X)\n",
        "\n",
        "    # Создаем DataFrame с user_id, предсказаниями и истинными значениями\n",
        "    results_df = pd.DataFrame({\n",
        "        'user_id': user_ids,\n",
        "        'prediction': predictions,\n",
        "        'true_rating': y\n",
        "    })\n",
        "\n",
        "    # Группируем по пользователям\n",
        "    ndcg_scores = []\n",
        "    for user_id, group in results_df.groupby('user_id'):\n",
        "        if len(group) < 2:\n",
        "            continue  # Пропускаем пользователей с менее чем 2 взаимодействиями\n",
        "\n",
        "        # Сортируем предсказания и истинные значения\n",
        "        pred_sorted = group.sort_values('prediction', ascending=False)['true_rating'].values\n",
        "        true_sorted = group.sort_values('true_rating', ascending=False)['true_rating'].values\n",
        "\n",
        "        # Вычисляем NDCG\n",
        "        ndcg = ndcg_score([true_sorted], [pred_sorted], k=k)\n",
        "        ndcg_scores.append(ndcg)\n",
        "\n",
        "    return np.mean(ndcg_scores) if ndcg_scores else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNUOQm8dnms4",
        "outputId": "8fa5b4ee-a5bf-40a8-a2cc-dccf0839f983"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing boosting data: 100%|██████████| 6038/6038 [01:37<00:00, 61.99it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LightGBM model...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.060908 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 32625\n",
            "[LightGBM] [Info] Number of data points in the train set: 43823, number of used features: 128\n",
            "[LightGBM] [Info] Start training from score 4.400338\n"
          ]
        }
      ],
      "source": [
        "boost_data = []\n",
        "boost_labels = []\n",
        "boost_user_ids = []  # Для хранения ID пользователей\n",
        "boost_timestamps = []  # Для хранения временных меток\n",
        "\n",
        "add_external = False\n",
        "\n",
        "with torch.no_grad():\n",
        "    for user_id in tqdm(ratings[\"user_id\"].unique(), desc=\"Preparing boosting data\"):\n",
        "        user_data = subset_data[subset_data['user_id'] == user_id]\n",
        "        user_items = user_data['item_id'].tolist()\n",
        "        user_ratings = user_data['rating'].tolist()\n",
        "        user_timestamps = user_data['timestamp'].tolist()\n",
        "\n",
        "        for i in range(1, len(user_items)):\n",
        "            # Получаем эмбеддинг пользователя\n",
        "            seq = user_items[:i]\n",
        "            if len(seq) > model.max_len:\n",
        "                seq = seq[-model.max_len:]\n",
        "            else:\n",
        "                seq = [0] * (model.max_len - len(seq)) + seq\n",
        "\n",
        "            input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            user_embedding = model.get_embeddings(input_seq)[0, -1, :].cpu().numpy()\n",
        "\n",
        "            # Получаем эмбеддинг айтема\n",
        "            item_id = user_items[i]\n",
        "            item_embedding = model.item_emb(torch.tensor([item_id], dtype=torch.long).to(device))[0].cpu().numpy()\n",
        "\n",
        "            # Получаем агрегированный эмбеддинг контекста\n",
        "            timestamp = user_data.iloc[i-1]['timestamp']\n",
        "            meta_features = np.array([user_id, item_id, timestamp])\n",
        "            n_meta_features = len(meta_features)\n",
        "\n",
        "            if add_external:\n",
        "                context_info = agg_emb_df[agg_emb_df['timestamp'] <= timestamp].iloc[-1]\n",
        "                mean_context = context_info['mean_embedding']\n",
        "                max_context = context_info['max_embedding']\n",
        "\n",
        "                # Объединяем все эмбеддинги\n",
        "                combined_features = np.concatenate([meta_features, user_embedding, mean_context, max_context, item_embedding])\n",
        "            else:\n",
        "                combined_features = np.concatenate([meta_features, user_embedding, item_embedding]) #mean_context, max_context, ])\n",
        "\n",
        "            boost_data.append(combined_features)\n",
        "            boost_labels.append(user_ratings[i])\n",
        "            boost_user_ids.append(user_id)\n",
        "            boost_timestamps.append(user_timestamps[i])\n",
        "\n",
        "# Преобразуем в numpy массивы\n",
        "X = np.array(boost_data)\n",
        "y = np.array(boost_labels)\n",
        "user_ids = np.array(boost_user_ids)\n",
        "train_users_boost = np.array([data[0] in train_users for data in X])\n",
        "\n",
        "# Разделяем на train/test\n",
        "# split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[train_users_boost, n_meta_features:], X[~train_users_boost, n_meta_features:]\n",
        "y_train, y_test = y[train_users_boost], y[~train_users_boost]\n",
        "user_ids_train, user_ids_test = user_ids[train_users_boost], user_ids[~train_users_boost]\n",
        "\n",
        "# Обучаем модель бустинга\n",
        "print(\"Training LightGBM model...\")\n",
        "lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.05)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Оцениваем модель с помощью RMSE\n",
        "train_pred = lgb_model.predict(X_train)\n",
        "test_pred = lgb_model.predict(X_test)\n",
        "\n",
        "train_rmse = np.sqrt(np.mean((train_pred - y_train) ** 2))\n",
        "test_rmse = np.sqrt(np.mean((test_pred - y_test) ** 2))\n",
        "\n",
        "print(f\"LightGBM Train RMSE: {train_rmse:.4f}\")\n",
        "print(f\"LightGBM Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "# Оцениваем модель с помощью NDCG\n",
        "train_ndcg = calculate_ndcg_for_boosting(lgb_model, X_train, y_train, user_ids_train, k=10)\n",
        "test_ndcg = calculate_ndcg_for_boosting(lgb_model, X_test, y_test, user_ids_test, k=10)\n",
        "\n",
        "print(f\"LightGBM Train NDCG@10: {train_ndcg:.4f}\")\n",
        "print(f\"LightGBM Test NDCG@10: {test_ndcg:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynbrLdTLnqp1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
